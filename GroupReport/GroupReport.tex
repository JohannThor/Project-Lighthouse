% v2-acmsmall-sample.tex, dated March 6 2012
% This is a sample file for ACM small trim journals
%
% Compilation using 'acmsmall.cls' - version 1.3 (March 2012), Aptara Inc.
% (c) 2010 Association for Computing Machinery (ACM)
%
% Questions/Suggestions/Feedback should be addressed to => "acmtexsupport@aptaracorp.com".
% Users can also go through the FAQs available on the journal's submission webpage.
%
% Steps to compile: latex, bibtex, latex latex
%
% For tracking purposes => this is v1.3 - March 2012

\documentclass[prodmode,acmtecs]{acmsmall} % Aptara syntax

% Package to generate and customize Algorithm as per ACM style
\usepackage[ruled]{algorithm2e}
\usepackage{textcomp}
\usepackage{gensymb}
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

% Metadata Information
% \acmVolume{9}
% \acmNumber{4}
% \acmArticle{39}
% \acmYear{2010}
% \acmMonth{3}

% Copyright
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

% DOI
%\doi{0000001.0000001}

%ISSN
%\issn{1234-56789}

% Document starts
\begin{document}

% Page heads
\markboth{}{Dynamic obstacle mapping for the visually impaired using sensor fusion.}

% Title portion
\title{Dynamic obstacle mapping for the visually impaired using sensor fusion.}
\author{Johann Thor Kristthorsson
\affil{University College London}
Ifeanyi Ndu
\affil{University College London}
Veselin Pavlov
\affil{University College London}
Shuang Zhang
\affil{University College London}
}
% NOTE! Affiliations placed here should be for the institution where the
%       BULK of the research was done. If the author has gone to a new
%       institution, before publication, the (above) affiliation should NOT be changed.
%       The authors 'current' address may be given in the "Author's addresses:" block (below).
%       So for example, Mr. Abdelzaher, the bulk of the research was done at UIUC, and he is
%       currently affiliated with NASA.

\begin{abstract}
Abstract goes here
\end{abstract}


%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
% \begin{CCSXML}
% <ccs2012>
%  <concept>
%   <concept_id>10010520.10010553.10010562</concept_id>
%   <concept_desc>Computer systems organization~Embedded systems</concept_desc>
%   <concept_significance>500</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10010520.10010575.10010755</concept_id>
%   <concept_desc>Computer systems organization~Redundancy</concept_desc>
%   <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10010520.10010553.10010554</concept_id>
%   <concept_desc>Computer systems organization~Robotics</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10003033.10003083.10003095</concept_id>
%   <concept_desc>Networks~Network reliability</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
% </ccs2012>  
% \end{CCSXML}

% \ccsdesc[500]{Computer systems organization~Embedded systems}
% \ccsdesc[300]{Computer systems organization~Redundancy}
% \ccsdesc{Computer systems organization~Robotics}
% \ccsdesc[100]{Networks~Network reliability}


\keywords{Sensor Fusion, Software Engineering,
Visually Impaired, Blind, Microsoft}

\acmformat{Johann Thor Kristthorsson, Ifeanyi Ndu, Veselin Pavlov and Shuang Zhang, 2016.}


\begin{bottomstuff}
This work was made in collaboration with Microsoft and the Guide Dogs association.
\end{bottomstuff}

\maketitle

\section{Introduction}
% Discuss prior work and the Cities Unlocked project briefly
We are working with Microsoft and the Guide Dogs Association to develop solutions for the visually impaired, to help them tackle the challenges of day to day tasks and empower them to participate in society in the same way as the non visually impaired.\\
The project falls under the Cities Unlocked 
%TODO: Citation
umbrella, an initiative that was started to respond to the lack of available guide dogs in the UK.\\
We aim to improve the experience of the members of the visually impaired population when entering specific indoor environments.
We will focus on environments that are unfamiliar to the user and that are not equipped with any specific navigation infrastructure beforehand.
We will do this by employing a range of wearable  sensors to build a dynamic map of the environment and give the user feedback on their surroundings.
\section{Literature review}
Obstacle detection is a large topic and has been a popular topic in the last few years with the advent of autonomous vehicles and unmanned aerial vehicles (UAVs).
The DARPA Grand Challenge in 2005 and the Urban Challenge 2007 offered millions of dollars in prizes and sparked great interest within the research community. ~\cite{DARPAGrandChallenge2005} ~\cite{DARPAUrbanChallenge2007}
In the specific case of obstacle detection as an assistive technology for the visually impaired there have been several projects. Cardin et al developed a wearable system that uses an array of ultrasonic transducers to do sonar sensing of obstacles around a subject. The subject is notified of the obstacles through vibrotactile instruments sown into clothing around the subjects torso. Experiments were conducted by blindfolding test subjects and having them navigate an environment filled with obstacles. The time it took the subjects to navigate was recorded and a the use of the system resulted in a 50\% reduction in navigation time after a short training time. ~\cite{Cardin2007} The system developed by Shin et al is similar but adds voice navigation through a headset. ~\cite{Shin2007} These kinds of reactive methods are common and widely explored both for the purposes of visually impaired navigation and for small robots or autonomous vehicles. ~\cite{Kato2002}
Shoval et al have made attempts to fuse the work done in autonomous driving and robotics with the work in assistive technologies. They did this by mounting sonar sensing hardware on a robot that serves the same purpose as a guide dog. It has one axle and wheels that are powered. The robot is pushed by the subject through a handle, the robot can then detect and avoid obstacles and the subject notices the change in the robots movements through the handle. ~\cite{Shoval2003}
As with the work of Cardin et al and Shin et al these robotics and autonomous driving works deal with the detection of obstacles in the immediate vicinity of the subject but do not persist or collect the location of the obstacle in a central location for further analysis and reuse.

Sensor fusion is a well known term in the context of obstacle detection and navigation. Labayrade et al describe algorithms and architectures that facilitate cooperative fusion of two different kinds of sensors, stereovision and LIDAR to detect obstacles in an autonomous driving scenario. ~\cite{Labayrade2005} Cooperative fusion is used there to detect obstacles in the road in front of a vehicle and determine its distance with good accuracy. The use of sensor fusion decreased the rate of false negatives in the detection, from 14.7\% and 5.2\% for LIDAR and stereovision respectively, down to 5.2\%. In addition the rate of false negatives went from 4.5\% and 3.2\% respectively down to 1.2\%. Cho et al reach similar results showing increase in true positive obstacle detections from 83.2\% to 89.9\% by using fusion instead of using raw sensor values.~\cite{Cho2014}

Therefore we proposed a project centered around gathering data from different sensors and using the collected data to determine the location of obstacles in a space.


\section{Proposed System}
\subsection{Preliminary work}
The initial plan for this project was to make use of hardware developed by students in the Electrical Engineering department. The hardware consists of an armband that can detect ultrasonic frequencies. Those frequencies are emitted by at least 3 beacons in predefined locations in the room. The armband, in conjunction with a computer and an Android device, then triangulates the position of the armband in space.

The use cases that were to be explore were centered around detecting gestures and possible indoor location possibilities.\\
However, soon after the project was presented to the team the hardware was found to have several major flaws. The team found that the location accuracy was considerably less than what was initially described. The initial description of the hardware stated that the location accuracy was sub-centimeter, which is the theoretical minimum, but the functional accuracy was between 10 and 50 centimeters.\\
This eliminates the gesture use case since more accuracy is needed to get accurate gesture recognition.\\
%TODO: citation
Another limitation of the hardware is that it only functions if 3 beacons are within a 130\degree cone in front of the receiver. This severely limits the usefulness of the device for indoor location since a wearable piece of hardware is usually blocked by the wearers body in addition to any obstacle in the environment.\\
%TODO: citation
Our initial contact with the client was a brainstorming session where he expressed his vision for the project. Ideas were then bounced around and a subject area was found that was open for research and of value to our client.\\
Microsoft Research have done some investigation into using Sensor Fusion
%TODO: citation
and wanted to see what applications for that technology in aiding the visually impaired.
After the meeting the team met several times to brainstorm and research possible applications and decided on a few possible project proposals.
These were presented to the client and he expressed interest in continuing with one of them.\\
For the final project proposal our client agreed on a research area that will help the visually impaired navigate an indoor environment.
Using sensor fusion the team was to build a system that can collect data from various disparate sensors and identify the location of the user and obstacles in it's environment.
The goals is to use cheap, of-the-shelf, sensors and mitigate any error in the sensor readings by correlating different readings. This will help us reduce error and increase accuracy by using various sensor fusion methods.
%TODO: citation 

\subsection{High level goals}


\subsection{Requirements}

\subsection{System Architecture}

\section{Implementation}

\subsection{Technology}

% Architecture here

\section{Project Management}
The team consists of 8 Masters students, 4 are doing a conversion course in Computer Science and 4 are doing Software Systems Engineering. As discussed earlier the 4 CS students are
\subsubsection{Processes}
Describe tools and processes, Scrum, Kanban etc.


\subsection{Tools}
Jira, Slack Maybe not needed.

\subsubsection{Communication}
Talk about our communication and meeting schedule.


\subsection{Testing strategy}
Show how our integration and load testing evaluates our architecture and a discussion of why it fits well

\section{Evaluation}
Metrics and graphs showing the performance of certain parts of the project.


\section{Lessons Learned} %Reflection
%\subsection{Challenges}
%Discuss what challenges the architecture faces and what can be done to mitigate them

\section{Life cycle and Future Work}
\subsection{Current state}
Describe the current state of the project\\
Capabilities, how many of the Goals and Requirements have been fulfilled.\\
Quality requirements and the tests we used to evaluate them\\

\subsection{Maintenance and Scaling}
Describe how the project can be maintained and scaled in the event of deployment.\\
Talk about the migration plan to Azure.\\

\section{Conclusions}


% Appendix
\appendix
\section*{APPENDIX}
\setcounter{section}{1}

\appendixhead{ZHOU}

% Acknowledgments
\begin{acks}
\end{acks}  

% Bibliography
\bibliographystyle{ACM-Reference-Format-Journals}
\bibliography{GroupReport-bibfile}

% History dates
%\received{February 2007}{March 2009}{June 2009}

% Electronic Appendix
\elecappendix

\medskip

\section{This is an example of Appendix section head}


\section{Appendix section head}

\end{document}
% End of v2-acmsmall-sample.tex (March 2012) - Gerry Murray, ACM


