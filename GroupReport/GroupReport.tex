% v2-acmsmall-sample.tex, dated March 6 2012
% This is a sample file for ACM small trim journals
%
% Compilation using 'acmsmall.cls' - version 1.3 (March 2012), Aptara Inc.
% (c) 2010 Association for Computing Machinery (ACM)
%
% Questions/Suggestions/Feedback should be addressed to => "acmtexsupport@aptaracorp.com".
% Users can also go through the FAQs available on the journal's submission webpage.
%
% Steps to compile: latex, bibtex, latex latex
%
% For tracking purposes => this is v1.3 - March 2012

\documentclass[prodmode,acmtecs]{acmsmall} % Aptara syntax

% Package to generate and customize Algorithm as per ACM style
\usepackage[ruled]{algorithm2e}
\usepackage{textcomp}
\usepackage{gensymb}
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

% Metadata Information
% \acmVolume{9}
% \acmNumber{4}
% \acmArticle{39}
% \acmYear{2010}
% \acmMonth{3}

% Copyright
%\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}

% DOI
%\doi{0000001.0000001}

%ISSN
%\issn{1234-56789}

% Document starts
\begin{document}

% Page heads
\markboth{}{Dynamic obstacle mapping for the visually impaired using sensor fusion.}

% Title portion
\title{Dynamic obstacle mapping for the visually impaired using sensor fusion.}
\author{Johann Thor Kristthorsson
\affil{University College London}
Ifeanyi Ndu
\affil{University College London}
Veselin Pavlov
\affil{University College London}
Shuang Zhang
\affil{University College London}
}
% NOTE! Affiliations placed here should be for the institution where the
%       BULK of the research was done. If the author has gone to a new
%       institution, before publication, the (above) affiliation should NOT be changed.
%       The authors 'current' address may be given in the "Author's addresses:" block (below).
%       So for example, Mr. Abdelzaher, the bulk of the research was done at UIUC, and he is
%       currently affiliated with NASA.

\begin{abstract}
Abstract goes here
\end{abstract}


%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%
% \begin{CCSXML}
% <ccs2012>
%  <concept>
%   <concept_id>10010520.10010553.10010562</concept_id>
%   <concept_desc>Computer systems organization~Embedded systems</concept_desc>
%   <concept_significance>500</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10010520.10010575.10010755</concept_id>
%   <concept_desc>Computer systems organization~Redundancy</concept_desc>
%   <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10010520.10010553.10010554</concept_id>
%   <concept_desc>Computer systems organization~Robotics</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>10003033.10003083.10003095</concept_id>
%   <concept_desc>Networks~Network reliability</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
% </ccs2012>  
% \end{CCSXML}

% \ccsdesc[500]{Computer systems organization~Embedded systems}
% \ccsdesc[300]{Computer systems organization~Redundancy}
% \ccsdesc{Computer systems organization~Robotics}
% \ccsdesc[100]{Networks~Network reliability}


\keywords{Sensor Fusion, Software Engineering,
Visually Impaired, Blind, Microsoft}

\acmformat{Johann Thor Kristthorsson, Ifeanyi Ndu, Veselin Pavlov and Shuang Zhang, 2016.}


\begin{bottomstuff}
This work was made in collaboration with Microsoft and the Guide Dogs association.
\end{bottomstuff}

\maketitle

\section{Introduction}
There are two million people living with visual impairment in the UK and despite that fact indoor environments are often not designed with this large group of people in mind. ~\cite{NHSBlindStatistics} This means that the visually impaired have to depend on mobility tools to make their way around their environment. Mobility canes and guide dogs are the most important ones but are limited in their use. The canes have limited use to a person needing to navigate from place to place and the guide dogs are not currently available to assist all visually impaired persons in the UK. The Guide Dogs Association is working towards the goal of providing as many people with guide dogs as possible but are not able to meet demand. The project falls under the Cities Unlocked umbrella, an initiative that was started to respond to the lack of available guide dogs in the UK ~\cite{CitiesUnlockedGeneral}. Cities Unlocked has recently been moving more towards improving and enriching the experiences of the visually impaired, rather than focusing solely on their mobility.

Microsoft has been exploring different technologies that could help the visually impaired in their day to day lives and wanted to elicit the help of UCL in this exploration. A team of 8 MSc students, 4 from software systems engineering and 4 from computer science, was put together and that team took Microsoft and the Guide Dogs Association on as clients for this project. To clarify the specific topic of the project several brainstorming sessions were held to explore the aims and goals. The specific field of study that the clients wanted the team to explore was the use of wearable sensors that could be used by the visually impaired. Additionally the clients wanted to use cheap, off the shelf, hardware and use a technique called Sensor Fusion to maintain accuracy.
After exploring several possible use cases for these technologies the client and the UCL team decided to find a way to improve the experience of the members of the visually impaired population when entering specific indoor environments.
Specifically in environments that are unfamiliar to the user and that are not equipped with any specific navigation infrastructure. There has already been thorough research done on indoor navigation but the identification of obstacles seemed to be a good fit for the hardware and technology requirements of the client.

\section{Literature review}
Obstacle detection is a large topic and has been popular in the last few years with the advent of autonomous vehicles and unmanned aerial vehicles.
The Defense Advanced Research Project Agency (DARPA) in the United States has held several competitions to push the research community towards this specific topic. The DARPA Grand Challenge in 2005 and the Urban Challenge in 2007 offered millions of dollars in prizes and sparked great interest within the research community ~\cite{DARPAGrandChallenge2005,DARPAUrbanChallenge2007}.
\subsection{Obstacle detection}
In the specific case of obstacle detection as an assistive technology there have been several papers exploring the topic. Cardin et al. developed a wearable system that uses an array of ultrasonic transducers to do sonar sensing of obstacles around a subject. The subject is notified of the obstacles through vibrotactile instruments sewn into clothing around the subjects torso. Experiments were conducted by blindfolding test subjects and having them navigate an environment filled with obstacles. The time it took the subjects to navigate was recorded and the use of the system resulted in a 50\% reduction in navigation time after a short training time ~\cite{Cardin2007}. The system developed by Shin et al. is similar to Cardin et al. but adds audible feedback through a headset ~\cite{Shin2007}. These methods are entirely reactive, in that they sense and identify obstacles in a space but do not record its position for possible future use. In addition to this the evaluation of the detection accuracy is lacking. The studies do not describe their experimental setups accurately and the data collection methods are never mentioned. 
%These types of methods are common and widely explored both for the purposes of small robots and autonomous vehicles ~\cite{Kato2002}. 
%Shoval et al. have made attempts to fuse the work done in autonomous driving and robotics with the work in assistive technologies. They did this by mounting sonar sensing hardware on a robot that serves the same purpose as a guide dog. It has one axle and wheels that are powered. The robot is pushed by the subject via a handle, the robot can then detect and avoid obstacles and the subject notices the change in the robots movements through the handle ~\cite{Shoval2003}.
\subsection{Sensor Fusion}
Sensor fusion, or cooperative fusion, is a well known term in the context of obstacle detection and navigation. Labayrade et al. describe algorithms and architectures that facilitate cooperative fusion of two different kinds of sensors, stereovision and LIDAR to detect obstacles in an autonomous driving scenario ~\cite{Labayrade2005}. Cooperative fusion is used to detect obstacles in the road in front of a vehicle and determine its distance with good accuracy. The use of sensor fusion decreased the rate of false negatives in the detection from 14.7\% and 5.2\% for LIDAR and stereovision respectively, down to 5.2\%. In addition the rate of false negatives went down from 4.5\% and 3.2\% respectively to 1.2\%. Cho et al. reached similar results showing an increase in the rate of true positives in obstacle detection from 83.2\% to 89.9\% by using fusion instead of using actual sensor values ~\cite{Cho2014}. Contrary to the research found for the use cases for the visually impaired these papers provide a thorough explanation of the evaluation and data gathering performed and will prove useful in designing the evaluation strategy for this project. These methods have been developed specifically for use in the scenario of autonomous driving but are directly applicable to the scenario of detecting obstacles around a visually impaired pedestrian.

In light of research described in the literature reviewed the team will be able to reach the aforementioned aims of this project by developing a data collection platform that gathers data from many different cheap sensors. The sensor data will be used to identify obstacles and the accuracy of the results will be increased by using sensor fusion.

\section{Proposed System}
\subsection{Preliminary work}
% Subsection this into VI obstacle detection and Sensor fusion, then bring back together.

%TODO: citation 

\subsection{High level goals}


\subsection{Requirements}
Below is a table showing a list of requirements for our sytem (please see Appendix xx for a full elaboration of the requirement pattern).

The table \ref{table:1} is an example of referenced \LaTeX elements.
 
\begin{table}[h!]
\centering
\begin{tabular}{||c c c c||} 
 \hline
 Col1 & Col2 & Col2 & Col3 \\ [0.5ex] 
 \hline\hline
 1 & 6 & 87837 & 787 \\ 
 2 & 7 & 78 & 5415 \\
 3 & 545 & 778 & 7507 \\
 4 & 545 & 18744 & 7560 \\
 5 & 88 & 788 & 6344 \\ [1ex] 
 \hline
\end{tabular}
\caption{Table to test captions and labels}
\label{table:1}
\end{table}

\subsection{System Architecture}

\section{Implementation}

\subsection{Technology}

% Architecture here

\section{Project Management}
The team consists of 8 Masters students, 4 are doing a conversion course in Computer Science and 4 are doing Software Systems Engineering. As discussed earlier the 4 CS students are
\subsubsection{Processes}
Describe tools and processes, Scrum, Kanban etc.


\subsection{Tools}
Jira, Slack Maybe not needed.

\subsubsection{Communication}
Talk about our communication and meeting schedule.


\subsection{Testing strategy}
Show how our integration and load testing evaluates our architecture and a discussion of why it fits well

\section{Evaluation}
Metrics and graphs showing the performance of certain parts of the project.


\section{Lessons Learned} %Reflection
%\subsection{Challenges}
%Discuss what challenges the architecture faces and what can be done to mitigate them

\section{Life cycle and Future Work}
\subsection{Current state}
Describe the current state of the project\\
Capabilities, how many of the Goals and Requirements have been fulfilled.\\
Quality requirements and the tests we used to evaluate them\\

\subsection{Maintenance and Scaling}
Describe how the project can be maintained and scaled in the event of deployment.\\
Talk about the migration plan to Azure.\\

\section{Conclusions}


% Appendix
\appendix
\section*{APPENDIX}
\setcounter{section}{1}

\appendixhead{ZHOU}

% Acknowledgments
\begin{acks}
\end{acks}  

% Bibliography
\bibliographystyle{ACM-Reference-Format-Journals}
\bibliography{GroupReport-bibfile}

% History dates
%\received{February 2007}{March 2009}{June 2009}

% Electronic Appendix
\elecappendix

\medskip

\section{This is an example of Appendix section head}


\section{Appendix section head}

\end{document}
% End of v2-acmsmall-sample.tex (March 2012) - Gerry Murray, ACM


